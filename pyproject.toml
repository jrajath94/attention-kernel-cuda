[build-system]
requires = ["setuptools>=68.0", "wheel", "torch>=2.0"]
build-backend = "setuptools.build_meta"

[project]
name = "attention-kernel-cuda"
version = "0.1.0"
description = "Custom CUDA Flash Attention kernels optimized for non-standard head dimensions"
readme = "README.md"
license = {text = "MIT"}
requires-python = ">=3.9"
authors = [
    {name = "Rajath John", email = "jrajath94@gmail.com"},
]
keywords = [
    "attention",
    "cuda",
    "flash-attention",
    "deep-learning",
    "pytorch",
    "transformer",
    "gpu",
    "kernel",
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "torch>=2.0",
    "numpy>=1.24",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "pytest-cov>=4.0",
    "ruff>=0.4.0",
    "mypy>=1.8",
]
bench = [
    "matplotlib>=3.7",
    "tabulate>=0.9",
]

[project.urls]
Homepage = "https://github.com/jrajath94/attention-kernel-cuda"
Repository = "https://github.com/jrajath94/attention-kernel-cuda"
Issues = "https://github.com/jrajath94/attention-kernel-cuda/issues"

[tool.setuptools.packages.find]
where = ["src"]

[tool.ruff]
target-version = "py310"
line-length = 100

[tool.ruff.lint]
select = ["E", "F", "W", "I", "N", "UP", "B", "SIM"]

[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --tb=short"
markers = [
    "cuda: marks tests requiring CUDA GPU",
    "slow: marks slow tests",
]
